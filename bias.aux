\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Griffioen:2015hta,Horbatsch:2016ilr,Higinbotham:2015rja}
\citation{}
\citation{Borkowski:1975}
\citation{Sick:2017aor}
\citation{Kraus:2014qua}
\citation{Hogg:2010yz}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Bias-Variance Trade-off In Proton Radius Extractions From Electron Scattering Data}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Bias}{1}{section*.4}}
\newlabel{sd}{{1}{1}{}{equation.2.1}{}}
\citation{Alberico:2008sz}
\citation{Hastie:2009}
\citation{Hastie:2009}
\newlabel{ztable}{{II}{2}{}{equation.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The following table shows the mean a$_0$ and radius terms from doing 1E6 Monte Carlo simulations for each range where Eq.\nobreakspace  {}\ref  {sd} was used to generate faux data in 0.05 fm$^{-2}$ steps with each points randomized using 0.5\% normal distribution. The results clearly indicate that the linear fits are biased. The input radius was 0.8113 fm (an a1/a0 term of 0.1097 fm$^{-1}$) and an a0 of one.}}{2}{table.1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Variance}{2}{section*.5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Goldilocks Delema}{2}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Bla bla bla.}}{2}{figure.1}}
\citation{}
\newlabel{fulltable}{{III}{3}{}{section*.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The input radius was 0.8113 fm (an a1/a0 of 0.1097 fm$^{-1}$).}}{3}{table.2}}
\newlabel{equaldatatable}{{III}{3}{}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Same as before, but now with equal number of data points of each range.}}{3}{table.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An illustration of the trade-off between bias and variance when selecting a statistical model. Simple models will have low variance but high bias (under-fitting) while complex models will have low bias but high variance (over-fitting). It is this trade-off that one seeks to balance. While with repeated Monte Carlo simulations it is trivial to find the optimal predictive model for a give set of data; in the real world true model is typically unknown and one only gets preform a very limited number of experiments and thus one relys on using real data and statistical methods for model selection\nobreakspace  {}\cite  {Hastie:2009}. }}{3}{figure.2}}
\newlabel{zoptimized}{{IV}{3}{}{equation.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Shown is the result of million simulations and fits of linear fits 0.1 -- 0.8\nobreakspace  {}fm$^{-2}$ and quadratic fits 0.1 -- 1.6\nobreakspace  {}fm$^{-2}$ both with 31 eqaully spaced data points. Using root mean square error as the matrix, neither example is significantly better then the other for exacting the proton radius This is annaligous to a dart game between two eqaully skilled players though one who hits the bulls eye more often yet has a large spread (low bias but high variance) and another equally skilled player who has a tigher cluster of hits but an offset (high bias but low variance).}}{3}{figure.3}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Model Selection and the Modern Debate}{3}{section*.7}}
\citation{Higinbotham:2015rja}
\citation{Kraus:2014qua}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Beyond Standard Dipole And Polynomial Fucntions}{4}{section*.8}}
\citation{Shmueli:2010}
\bibdata{biasNotes,elastic}
\bibcite{Griffioen:2015hta}{{1}{2016}{{Griffioen\ \emph  {et~al.}}}{{Griffioen, Carlson,\ and\ Maddox}}}
\bibcite{Horbatsch:2016ilr}{{2}{2017}{{Horbatsch\ \emph  {et~al.}}}{{Horbatsch, Hessels,\ and\ Pineda}}}
\bibcite{Higinbotham:2015rja}{{3}{2016}{{Higinbotham\ \emph  {et~al.}}}{{Higinbotham, Kabir, Lin, Meekins, Norum,\ and\ Sawatzky}}}
\bibcite{Borkowski:1975}{{4}{1975}{{Borkowski\ \emph  {et~al.}}}{{Borkowski, Simon, Walther,\ and\ Wendling}}}
\bibcite{Sick:2017aor}{{5}{2017}{{Sick\ and\ Trautmann}}{{}}}
\bibcite{Kraus:2014qua}{{6}{2014}{{Kraus\ \emph  {et~al.}}}{{Kraus, Mesick, White, Gilman,\ and\ Strauch}}}
\bibcite{Hogg:2010yz}{{7}{2010}{{Hogg\ \emph  {et~al.}}}{{Hogg, Bovy,\ and\ Lang}}}
\bibcite{Hastie:2009}{{8}{2009}{{Hastie\ \emph  {et~al.}}}{{Hastie, Tibshirani,\ and\ Friedman}}}
\bibcite{Shmueli:2010}{{9}{2010}{{Shmueli}}{{}}}
\bibstyle{apsrev4-1}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Appliction To Experimental Data}{5}{section*.9}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Summary}{5}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{5}{section*.11}}
\newlabel{LastBibItem}{{9}{5}{}{section*.11}{}}
\newlabel{LastPage}{{}{5}{}{}{}}
